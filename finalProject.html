<!-- Luisa Zorzetto - CTS 1000 -->

<!-- oedAssignment.html -->
<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Final Project</title>
        <link rel="stylesheet" href="webstyle.css">
    </head>
    <body>

        <!-- Header -->
        <header>
            <h1>Luisa Zorzetto's Portfolio</h1>
            <!-- Navigation -->
            <nav>
                <a href="index.html">Portfolio Home</a>
                <a href="CTS1000.html">CTS*1000</a>
            </nav>
        </header>

        <!-- Main Content -->
        <main id="main-content">
            <section class="info">
                <br>
                <h1 style= "color: #854C4C;">Final Project</h1>
                <br>

                <div class="list-container-2" style="text-align: center;">
                <h2>Proposal</h2>
                <p style="text-align: left;">
                    <b>Topic:</b> Gender bias in medicine 
                    <br><b>Keyword:</b> Data, Code and Algorithms
                    <br><b>Question:</b> What effects do gender biases have on the design and behaviours of technological medical systems?
                    <br><b>Form:</b> Essay
                    <br>

                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The relationship between culture and technology is evident throughout modern society. An area with a clear example of where this connection appears is in the creation of technologies. The information that powers our technology is a reflection of the people producing it. The topic of “gender bias in algorithms” is a broad topic, so looking into a specific field, the medical field, the scope of this assignment can be narrowed. Therefore, leading to the question, what effects do gender biases have on the design and behaviours of technological medical systems?
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Historically, medical data used to influence treatments or symptom detection was collected mostly from men. These datasets became commonly used and accepted as “typical” markers in patients. This led to men being prioritized in medicine, even before the use of AI and medical algorithms. In the medical field, societal assumptions, like these markers, are embedded into the data used to train and develop medical systems, leading to the same detection and treatment plans for both genders.  When studying data, code, and algorithms the topic of gender bias is one that requires further study and research. Gender biases in medical data are dangerous as they often lead to under-detected symptoms, misinterpreted data, or fewer effective procedures on women as the methodology is predominantly developed with male datasets. This topic is important since medical systems have a direct impact on our society’s health and well-being. To determine the reasoning for this bias, it is necessary to look at both technical and cultural factors, such as who is gathering the data, what is being collected, and what is deemed “standard”. To analyse this question, I will look at three components: how gender data gaps occur, how gender data gaps enter algorithms, and examples of medical technologies that are affected by gender bias. 
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The form of this project will be an essay as this is the most appropriate format to connect theories and examples of this argument. An essay format will allow for a structured explanation and the opportunity to connect the three components above. Through this form, the exploration of how gender data gaps in medicine have developed, and their consequences in healthcare, will be analyzed with evidence and research to link the cultural and technical relationships in society. 
                </p>
                
                <h3><br><br><b>Sources</b></h3>
                
                <p style="text-align: left;">
                    <br>Cirillo, D., Catuara-Solarz, S., Morey, C., Guney, E., Subirats, L., Mellino, S., Gigante, A., Valencia, A., 
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rementeria, M. J., Chadha, A. S., & Mavridis, N. (2020). Sex and gender differences and biases in artificial 
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;intelligence for biomedicine and healthcare. NPJ Digital Medicine, 3(1), Article 81. 
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://doi.org/10.1038/s41746-020-0288-5">https://doi.org/10.1038/s41746-020-0288-5</a>
                    <br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This source argues that differences in sex and gender are often overlooked by artificial intelligence systems created for medical care, which leads to algorithms generating inaccurate or biases results. The authors of this work use a method of medicine known as precision medicine, which uses an understanding of differences in health due to genetic and/or environmental factors. The research shows that AI algorithms are learning from datasets that either ignore sex and gender or underrepresent women. An extremely important finding from this source is how medical technologies refer to symptoms/characteristics in men as the “norm,” which leads to discriminatory medical outcomes for women. Something that could be missing from this source is why gender gaps are common in data collection, which is the primary cause for biased systems. This is a relevant source for my project as is discusses how gender gaps enter code for medical systems, and it relates the concept of medical algorithms to real world consequences. 

                    <br>
                    <br>D’Ignazio, C., & Klein, L. (2020). Introduction: Why Data Science Needs Feminism. In Data Feminism. Retrieved 
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from <a href="https://data-feminism.mitpress.mit.edu/pub/frfa9szd">https://data-feminism.mitpress.mit.edu/pub/frfa9szd</a>
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The introduction chapter of Data Feminism discusses the seven principles that are necessary for making informed thoughts to challenge and address inequalities in data science. The authors criticize that data science frequently embeds different dimensions of identity such as social, race and gender inequalities into collection and interpretation of data. An important detail in the chapter the need for all lived experiences to be valued and for data feminism in society, which are necessary in growing my understanding of how data should be collected for datasets in the medical field. It is lacking a specific focus as this is a broad chapter of ideas that can be implemented in a variety of fields. This source is relevant to support the challenging of gender bias in medical datasets which I will be exploring in my project. 

                    <br>
                    <br>D’Ignazio, C., & Klein, L. (2020). 1. The Power Chapter. In Data Feminism. Retrieved from 
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href ="https://data-feminism.mitpress.mit.edu/pub/vi8obxh7">https://data-feminism.mitpress.mit.edu/pub/vi8obxh7</a>
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chapter one of Data Feminism is important in the argument that technology and data are not neutral and have biases and power inequality built in them. An important takeaway from this chapter is a term the authors explain to describe data teams made up of people from the dominant group (in this case men). The term is, “privilege hazard”, which can be described as the exclusion of other identities and perspectives. They argue that by not having diverse data or software teams, underrepresented groups are excluded from datasets, leading to imbalances and inaccurate data. While the source does touch on both artificial intelligence systems and the medical field, it does not connect them in the way I want to for my project. However, this chapter can help me examine how stereotypes are built into systems, the reasoning behind unreliable datasets, and will allow me to connect the two based in the medical field. 

                    <br>
                    <br>Ford, P. (2015, June 11) What is code? . Bloomberg. Retrieved from 
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href ="www.bloomberg.com/graphics/2015-paul-ford-what-is-code/.">www.bloomberg.com/graphics/2015-paul-ford-what-is-code/.</a>
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This article from Bloomberg explains what code and algorithms are, how they are created, and what decisions programmers make to build software systems. This article is critical as it shows how the people who write code are reflected in these systems and how functional they are to the users. An important detail from Paul Ford’s explanation is when he discusses who becomes programmers and software industry norms. Although this article does not directly talk about gender bias in medicine, it proves that human decisions are the reason these algorithms make the assumptions and behave the way they do. What may be missing from this article is the creation of datasets and why they are missing information on women’s health. This article will help my essay explain how and why injustices can occur in programming and why it is important to have a diverse development team.  

                    <br>
                    <br>Oxford University Press. (n.d.). Bias, n., adj., & adv. In Oxford English dictionary. Retrieved from  
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href ="https://doi.org/10.1093/OED/8048838144">https://doi.org/10.1093/OED/8048838144</a>
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The OED will provide meanings for the word “bias” in different scenarios, as well as its historical development. Important details that can be found in this source include the evolution of the word’s meaning and how it can be used in scientific context. A limitation is that the source doesn’t define algorithmic bias. However, the OED can be useful for defining what bias is, which aids in defining gender bias in relation to medical data collected and algorithms. This source is relevant as it will provide my project with a clear definition and understanding of bias, which will support the clarity of my argument.  
                </p>
                </div>

            </section>
        </main>

        <!-- Footer -->
            <footer>
                <p>Created by Luisa Zorzetto</p>
                <p>Email: lzorzett@uoguelph.ca</p>
            </footer>

    </body>
</html>